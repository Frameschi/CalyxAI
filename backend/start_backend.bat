@echo off
echo ğŸš€ Iniciando Calyx AI Backend con DeepSeek-R1...
echo.

cd /d "%~dp0"

echo ğŸ“¦ Verificando dependencias de Python...
python -c "import ollama, fastapi, torch; print('âœ… Dependencias OK')" 2>nul
if errorlevel 1 (
    echo âŒ Faltan dependencias. Ejecuta: pip install -r requirements.txt
    pause
    exit /b 1
)

echo ğŸ”§ Verificando Ollama service...
curl -s http://localhost:11434/api/tags >nul 2>&1
if errorlevel 1 (
    echo âŒ Ollama no estÃ¡ ejecutÃ¡ndose. Inicia Ollama primero.
    echo ğŸ’¡ Ejecuta: ollama serve
    pause
    exit /b 1
)

echo âœ… Ollama service activo

echo ğŸ¤– Verificando modelo DeepSeek-R1...
python -c "import ollama; client=ollama.Client(); models=client.list(); model_names=[m.model for m in models.models]; print('Modelos:', model_names); assert 'hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q4_K_M' in model_names" 2>nul
if errorlevel 1 (
    echo âŒ Modelo DeepSeek-R1 no encontrado. Descargando...
    ollama pull hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q4_K_M
    if errorlevel 1 (
        echo âŒ Error al descargar modelo
        pause
        exit /b 1
    )
)

echo âœ… Modelo DeepSeek-R1 listo

echo ğŸš€ Iniciando servidor FastAPI...
python main.py